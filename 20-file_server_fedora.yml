---
- name: Configure container server
  hosts: file_server
  become: true  # Run with sudo privileges

### Set variables

  vars:
    ansible_remote_tmp: /tmp/.ansible/tmp

  vars_files:
    - group_vars/file_server/secrets.yml
    - group_vars/file_server/common.yml

### Run all tasks
  tasks:

## Create paths and mount all filesystems

    - name: Create required directories (excluding Btrfs-managed paths)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /swap      # Mountpoint for swapfile
        - /srv/nvme  # Base for NVMe subvolumes
        - /srv/mnt   # Directory for remote NFS mounts
        
    - name: Detect current /home mount (if any)
      ansible.builtin.command: findmnt -no SOURCE,FSTYPE,OPTIONS /home
      register: home_findmnt
      changed_when: false
      failed_when: false

    - name: Remove Fedora Cloud /home entry from /etc/fstab (Btrfs subvolume)
      ansible.builtin.lineinfile:
        path: /etc/fstab
        regexp: '^\S+\s+/home\s+btrfs\s+.*subvol=.*'
        state: absent
        backup: yes

    - name: "Check if /home is still mounted"
      ansible.builtin.command: mountpoint -q /home
      register: home_still_mounted
      changed_when: false
      failed_when: false

    - name: "Fallback: lazy-unmount /home if still busy"
      ansible.builtin.command:
        argv: ["umount", "-l", "/home"]
      when: home_still_mounted.rc == 0
      failed_when: false

    - name: Ensure /home directory exists (empty mountpoint)
      ansible.builtin.file:
        path: /home
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Mount Btrfs subvolumes (NVMe U.2 Drive)
      ansible.builtin.mount:
        path: "{{ item.path }}"
        src: "UUID=19583b5d-8c79-4f16-b902-153124679f5e"
        fstype: btrfs
        opts: "{{ item.opts }}"
        state: mounted
      loop:
        - { path: "/swap", opts: "subvol=@swap,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd" }
        - { path: "/home", opts: "subvol=@home,noatime,nodev,nosuid,compress=zstd,autodefrag,space_cache=v2,ssd" }
        - { path: "/srv/nvme", opts: "subvol=@srv_nvme,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd" }
        - { path: "/home/podman/data", opts: "subvol=^podman_data,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd" }
        - { path: "/home/podman/data/nextcloud", opts: "subvol=^podman_data_nextcloud,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd" }
        - { path: "/home/podman/data/sabnzbd", opts: "subvol=^podman_data_sabnzbd,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd" }

    - name: Mount Btrfs root NVMe U.2 Drive
      ansible.builtin.mount:
        path: "{{ item.path }}"
        src: "UUID={{ item.uuid }}"
        fstype: btrfs
        opts: "{{ item.opts }}"
        state: mounted
      loop:
        - { path: "/srv/nvme/btrfsroot", uuid: "19583b5d-8c79-4f16-b902-153124679f5e", opts: "noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd" }

    - name: Ensure all mounts are persistent in /etc/fstab
      ansible.builtin.lineinfile:
        path: /etc/fstab
        line: "{{ item }}"
        state: present
      loop:
        - "UUID=19583b5d-8c79-4f16-b902-153124679f5e /swap btrfs subvol=@swap,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd 0 0"
        - "UUID=19583b5d-8c79-4f16-b902-153124679f5e /home btrfs subvol=@home,noatime,nodev,nosuid,compress=zstd,autodefrag,space_cache=v2,ssd 0 0"
        - "UUID=19583b5d-8c79-4f16-b902-153124679f5e /srv/nvme btrfs subvol=@srv_nvme,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd 0 0"
        - "UUID=19583b5d-8c79-4f16-b902-153124679f5e /srv/nvme/btrfsroot btrfs noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd 0 0"
        - "UUID=19583b5d-8c79-4f16-b902-153124679f5e /home/podman/data btrfs subvol=^podman_data,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd 0 0"
        - "UUID=19583b5d-8c79-4f16-b902-153124679f5e /home/podman/data/nextcloud btrfs subvol=^podman_data_nextcloud,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd 0 0"
        - "UUID=19583b5d-8c79-4f16-b902-153124679f5e /home/podman/data/sabnzbd btrfs subvol=^podman_data_sabnzbd,noatime,nodev,nosuid,noexec,compress=zstd,autodefrag,space_cache=v2,ssd 0 0"
        - "/swap/swapfile none swap defaults 0 0"
        
## Set FQDN

    - name: Set hostname with hostnamectl
      ansible.builtin.command:
        cmd: "hostnamectl set-hostname {{ fqdn }}"
      changed_when: true

    - name: Ensure /etc/hosts contains proper FQDN mapping
      ansible.builtin.lineinfile:
        path: /etc/hosts
        regexp: "^127\\.0\\.1\\.1\\s+"
        line: "127.0.1.1 {{ fqdn }} {{ hostname }}"
        state: present
        create: yes

    - name: Ensure /etc/hostname contains only short hostname
      ansible.builtin.copy:
        content: "{{ hostname }}\n"
        dest: /etc/hostname
        owner: root
        group: root
        mode: '0644'

## install packages

    - name: Install required packages (Fedora)
      ansible.builtin.dnf:
        name:
          - ca-certificates
          - curl
          - gpg
          - htop
          - iotop
          - strace
          - lsof
          - mc
          - audit                  
          - btrfs-progs
          - git
          - rsync
          - plocate
          - jq
          - fastfetch              
          - firewalld
          - podman
          - podman-compose
          - slirp4netns
          - python3-podman
          - openssh-clients        
          - systemd-container
          - selinux-policy-targeted
          - policycoreutils
          - policycoreutils-python-utils
          - setools-console
          - checkpolicy
          - python3-bcrypt         
        state: present
        update_cache: yes

## Configure SELINUX

    # Ensure SELinux is enforcing with the targeted policy (idempotent).
    # The module reports if a reboot/relabel is required (e.g., if SELinux was disabled).
    - name: Ensure SELinux is enforcing (targeted)
      ansible.posix.selinux:
        policy: targeted
        state: enforcing
      register: selinux_result

    - name: Schedule full filesystem relabel if required
      ansible.builtin.file:
        path: /.autorelabel
        state: touch
      when: selinux_result.reboot_required | default(false)

    - name: Reboot to relabel if required
      ansible.builtin.reboot:
        reboot_timeout: 3600
      when: selinux_result.reboot_required | default(false)

    - name: Verify SELinux is enabled and enforcing
      ansible.builtin.command: getenforce
      register: getenforce_out
      changed_when: false

    - name: Switch to enforcing at runtime (if not already)
      ansible.builtin.command: setenforce 1
      when: getenforce_out.stdout != "Enforcing"
      failed_when: false

    # Podman: enable labeling and remind to use :Z on volume mounts
    - name: Ensure containers.conf enables labeling
      ansible.builtin.copy:
        dest: /etc/containers/containers.conf
        mode: '0644'
        content: |
          [containers]
          label = true

    - name: Reminder for Podman volumes
      ansible.builtin.debug:
        msg: >
          Use ':Z' on volume mounts in compose files (e.g. './data:/var/lib/nextcloud:Z')
          so host paths get the correct SELinux label automatically.    

## configue firewalld

    - name: Ensure Firewalld is running and enabled
      ansible.builtin.systemd:
        name: firewalld
        enabled: yes
        state: started

    - name: Check existing Firewalld zones
      ansible.builtin.command: "firewall-cmd --get-zones"
      register: firewalld_existing_zones
      changed_when: false

    - name: Create Firewalld zones if missing
      ansible.builtin.firewalld:
        zone: "{{ item }}"
        permanent: yes
        state: present
      loop:
        - servers
        - 10G
        - containers
        - nfs
        - smb
      when: "item not in firewalld_existing_zones.stdout"

    - name: Reload Firewalld to apply new zones
      ansible.builtin.command:
        cmd: "firewall-cmd --reload"
      changed_when: false

    - name: Bind interfaces to Firewalld zones
      ansible.builtin.firewalld:
        zone: "{{ item.zone }}"
        interface: "{{ item.interface }}"
        permanent: yes
        immediate: yes
        state: enabled
      loop:
        - { zone: "servers",    interface: "eth0" }
        - { zone: "10G",        interface: "eth1" }
        - { zone: "containers", interface: "eth2" }
        - { zone: "nfs",        interface: "eth3" }
        - { zone: "smb",        interface: "eth4" }

    - name: Allow required services in Firewalld zones
      ansible.builtin.firewalld:
        zone: "{{ item.zone }}"
        service: "{{ item.service }}"
        permanent: yes
        immediate: yes
        state: enabled
      loop:
        - { zone: "servers", service: "ssh" }
        - { zone: "servers", service: "samba" }
        - { zone: "servers", service: "dns" }
        - { zone: "servers", service: "dhcp" }
        - { zone: "servers", service: "ntp" }
        - { zone: "10G",    service: "samba" }
        - { zone: "10G",    service: "http" }
        - { zone: "10G",    service: "https" }
        - { zone: "10G",    service: "dns" }
        - { zone: "10G",    service: "dhcp" }
        - { zone: "10G",    service: "ntp" }
        - { zone: "nfs",    service: "nfs" }
        - { zone: "nfs",    service: "mountd" }
        - { zone: "nfs",    service: "rpc-bind" }
        - { zone: "smb",    service: "samba" }

    - name: Open required ports in Firewalld zones
      ansible.builtin.firewalld:
        zone: "{{ item.zone }}"
        port: "{{ item.port }}"
        permanent: yes
        immediate: yes
        state: enabled
      loop:
        - { zone: "nfs",        port: "32767/tcp" }
        - { zone: "nfs",        port: "32767/udp" }
        - { zone: "nfs",        port: "32765/tcp" }
        - { zone: "nfs",        port: "32765/udp" }
        - { zone: "containers", port: "8053/tcp" }
        - { zone: "containers", port: "8053/udp" }
        - { zone: "containers", port: "8080/tcp" }
        - { zone: "containers", port: "8081/tcp" }
        - { zone: "containers", port: "8082/tcp" }
        - { zone: "containers", port: "8083/tcp" }
        - { zone: "containers", port: "8084/tcp" }

    - name: Enable IP forwarding
      ansible.builtin.sysctl:
        name: net.ipv4.ip_forward
        value: '1'
        sysctl_set: yes
        state: present
        reload: yes

    - name: Redirect common ports for rootless podman (rich rules)
      ansible.posix.firewalld:
        zone: "{{ item.zone }}"
        rich_rule: 'rule family="ipv4" forward-port port="{{ item.src_port }}" protocol="{{ item.protocol }}" to-port="{{ item.to_port }}"'
        permanent: yes
        immediate: yes
        state: enabled
      loop:
        - { zone: "servers", src_port: 53,  protocol: "tcp", to_port: 8053 }
        - { zone: "servers", src_port: 53,  protocol: "udp", to_port: 8053 }
        - { zone: "servers", src_port: 67,  protocol: "tcp", to_port: 8067 }
        - { zone: "servers", src_port: 67,  protocol: "udp", to_port: 8067 }
        - { zone: "servers", src_port: 123, protocol: "udp", to_port: 8123 }
        - { zone: "10G",    src_port: 80,  protocol: "tcp", to_port: 8080 }
        - { zone: "10G",    src_port: 443, protocol: "tcp", to_port: 8443 }
        - { zone: "10G",    src_port: 53,  protocol: "tcp", to_port: 8053 }
        - { zone: "10G",    src_port: 53,  protocol: "udp", to_port: 8053 }
        - { zone: "10G",    src_port: 67,  protocol: "tcp", to_port: 8067 }
        - { zone: "10G",    src_port: 67,  protocol: "udp", to_port: 8067 }
        - { zone: "10G",    src_port: 123, protocol: "udp", to_port: 8123 }

    - name: Reload Firewalld to apply changes
      ansible.builtin.command:
        cmd: "firewall-cmd --reload"

## Set password and keys for admin user

    - name: Set michael’s Linux password
      ansible.builtin.user:
        name: michael
        password: "{{ michael_password | password_hash('sha512') }}"

    - name: Check if SSH private key exists for user michael
      ansible.builtin.stat:
        path: /home/michael/.ssh/id_rsa
      register: michael_ssh_key

    - name: Create .ssh directory for michael (if needed)
      ansible.builtin.file:
        path: /home/michael/.ssh
        state: directory
        owner: michael
        group: michael
        mode: '0700'

    - name: Generate SSH key for michael if it doesn't exist
      become_user: michael
      community.crypto.openssh_keypair:
        path: /home/michael/.ssh/id_rsa
        type: rsa
        size: 4096
        passphrase: "{{ ssh_key_passphrase }}"
        comment: "michael@{{ inventory_hostname }}"
      when: not michael_ssh_key.stat.exists

    - name: Read public SSH key
      ansible.builtin.slurp:
        src: /home/michael/.ssh/id_rsa.pub
      register: michael_public_key
      become_user: michael

    - name: Create /home/michael/build directory with proper ownership
      ansible.builtin.file:
        path: /home/michael/build
        state: directory
        owner: michael
        group: michael
        mode: '0755'

## Create bash aliases for root

    - name: Ensure aliases are present in .bashrc for root
      ansible.builtin.lineinfile:
        path: "{{ item.bashrc }}"
        line: "{{ item.alias }}"
        create: yes
        owner: "{{ item.user }}"
        group: "{{ item.user }}"
        mode: '0644'
        insertafter: EOF
      loop:
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias gs='git status'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias c='clear'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias h='history'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias ls='ls -alh --color=auto'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias df='df -h'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias ports='ss -tulpn'" }

## Configure fastfetch for running after login

    - name: Make fastfetch run for every interactive shell
      ansible.builtin.copy:
        dest: /etc/profile.d/fastfetch.sh
        owner: root
        group: root
        mode: '0755'
        content: |
          #!/bin/sh
          # Only run in an interactive terminal
          if [ -t 1 ] && command -v fastfetch >/dev/null 2>&1; then
            fastfetch
          fi

## Configure podman user for running rootless containers

    - name: Ensure user 'podman' exists with a specific password
      ansible.builtin.user:
        name: podman
        shell: /bin/bash
        home: /home/podman

    - name: Set podman’s Linux password
      ansible.builtin.user:
        name: podman
        password: "{{ podman_password | password_hash('sha512') }}"

    - name: Ensure podman has authorized SSH key
      ansible.posix.authorized_key:
        user: podman
        state: present
        key: "{{ podman_ssh_pubkey }}"

    - name: Get podman user's UID
      ansible.builtin.command: id -u podman
      register: podman_uid_cmd
      changed_when: false

    - name: Set podman_uid fact
      ansible.builtin.set_fact:
        podman_uid: "{{ podman_uid_cmd.stdout }}"

    - name: Get podman user's GID
      ansible.builtin.command: id -g podman
      register: podman_gid_cmd
      changed_when: false

    - name: Set podman_gid fact
      ansible.builtin.set_fact:
        podman_gid: "{{ podman_gid_cmd.stdout }}"

    - name: Enable linger for podman user so socket survives reboot
      ansible.builtin.command: loginctl enable-linger podman

    - name: Ensure podman.socket is enabled and running for podman user
      ansible.builtin.command: >
        sudo -u podman env XDG_RUNTIME_DIR=/run/user/{{ podman_uid }}
        systemctl --user enable --now podman.socket

    - name: Ensure podman's bashrc exports DOCKER_HOST
      ansible.builtin.lineinfile:
        path: /home/podman/.bashrc
        line: 'export DOCKER_HOST=unix://$XDG_RUNTIME_DIR/podman/podman.sock'
        create: yes
        owner: podman
        group: podman
        mode: '0644'

    - name: Create systemd user unit directory
      ansible.builtin.file:
        path: /home/podman/.config/systemd/user
        state: directory
        owner: podman
        group: podman
        mode: '0755'

## Set podman registries

    - name: Deploy podman registries file
      ansible.builtin.template:
        src: file_server/podman/registries.conf.j2
        dest: /etc/registries.conf
        owner: root
        group: root
        mode: '0644'

    - name: Deploy podman user registries file
      ansible.builtin.template:
        src: file_server/podman/podman_registries.conf.j2
        dest: /home/podman/.config/containers/registries.conf
        owner: podman
        group: podman
        mode: '0644'

## Deploy podman containers

# NTP

    - name: Deploy NTP compose file
      ansible.builtin.template:
        src: file_server/compose/ntp/compose.yml.j2
        dest: /home/podman/compose/ntp/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start NTP
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/ntp.service
        content: |
          [Unit]
          Description=NTP

          [Service]
          WorkingDirectory=/home/podman/compose/ntp
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Nginx Proxy Manager

    - name: Deploy npm compose file
      ansible.builtin.template:
        src: file_server/compose/npm/compose.yml.j2
        dest: /home/podman/compose/npm/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Deploy npm env file
      ansible.builtin.template:
        src: file_server/compose/npm/.env.j2
        dest: /home/podman/compose/npm/.env
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Nginx Proxy Manager
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/npm.service
        content: |
          [Unit]
          Description=Nginx Proxy Manager

          [Service]
          WorkingDirectory=/home/podman/compose/npm
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Heimdall

    - name: Deploy Heimdall compose file
      ansible.builtin.template:
        src: file_server/compose/heimdall/compose.yml.j2
        dest: /home/podman/compose/heimdall/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Heimdall
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/heimdall.service
        content: |
          [Unit]
          Description=Heimdall

          [Service]
          WorkingDirectory=/home/podman/compose/heimdall
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Pihole-clients

    - name: Deploy Pihole-clients compose file
      ansible.builtin.template:
        src: file_server/compose/pihole-clients/compose.yml.j2
        dest: /home/podman/compose/pihole-clients/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Deploy Pihole-clients env file
      ansible.builtin.template:
        src: file_server/compose/pihole-clients/.env.j2
        dest: /home/podman/compose/pihole-clients/.env
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Pihole-clients
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/pihole-clients.service
        content: |
          [Unit]
          Description=Pihole-clients

          [Service]
          WorkingDirectory=/home/podman/compose/pihole-clients
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Pihole-servers

    - name: Deploy Pihole-servers compose file
      ansible.builtin.template:
        src: file_server/compose/pihole-servers/compose.yml.j2
        dest: /home/podman/compose/pihole-servers/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Deploy Pihole-servers env file
      ansible.builtin.template:
        src: file_server/compose/pihole-servers/.env.j2
        dest: /home/podman/compose/pihole-servers/.env
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Pihole-servers
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/pihole-servers.service
        content: |
          [Unit]
          Description=Pihole-clients

          [Service]
          WorkingDirectory=/home/podman/compose/pihole-servers
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Nextcloud

    - name: Deploy Nextcloud compose file
      ansible.builtin.template:
        src: file_server/compose/nextcloud/compose.yml.j2
        dest: /home/podman/compose/nextcloud/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Deploy Nextcloud env file
      ansible.builtin.template:
        src: file_server/compose/nextcloud/.env.j2
        dest: /home/podman/compose/nextcloud/.env
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Nextcloud
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/nextcloud.service
        content: |
          [Unit]
          Description=Nextcloud

          [Service]
          WorkingDirectory=/home/podman/compose/nextcloud
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

    - name: Deploy Nextcloud config.php file
      ansible.builtin.template:
        src: file_server/compose/nextcloud/config.php.j2
        dest: /home/podman/compose/nextcloud/config.php
        owner: podman
        group: podman
        mode: '0644'

# ## Reload systemd and enable/start containers

#     - name: Reload systemd user units for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user daemon-reload'

# # NTP Server

#     - name: Enable NTP service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable ntp.service'

# # Nginx Proxy Manager

#     - name: Enable NPM service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable npm.service'

# # Heimdall

#     - name: Enable Heimdall service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable heimdall.service'

# # Pihole-clients

#     - name: Enable Pihole-clients service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable pihole-clients.service'

# # Pihole-servers

#     - name: Enable Pihole-servers service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable pihole-servers.service'

# Nextcloud

    # - name: Enable Nextcloud service for podman
    #   ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable nextcloud.service'

    # - name: Enable cron timer
    #   ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable nextcloud-cron.timer'

## Finishing deployment by rebooting the server

    - name: Reboot the server
      ansible.builtin.reboot:
        reboot_timeout: 600     # wait up to 10 minutes for reboot
        test_command: whoami    # command to verify SSH is back
      tags: reboot

    - name: Reset SSH connection after reboot
      meta: reset_connection
      tags: reboot
