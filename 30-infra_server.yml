---
- name: Configure container server
  hosts: container_server
  become: true  # Run with sudo privileges

### Set variables

  vars_files:
    - group_vars/container_server/secrets.yml
    - group_vars/container_server/common.yml

### Run all tasks
  tasks:

    ## Set FQDN

    - name: Set hostname with hostnamectl
      ansible.builtin.command:
        cmd: "hostnamectl set-hostname {{ fqdn }}"
      changed_when: true

    - name: Ensure /etc/hosts contains proper FQDN mapping
      ansible.builtin.lineinfile:
        path: /etc/hosts
        regexp: "^127\\.0\\.1\\.1\\s+"
        line: "127.0.1.1 {{ fqdn }} {{ hostname }}"
        state: present
        create: yes

    - name: Ensure /etc/hostname contains only short hostname
      ansible.builtin.copy:
        content: "{{ hostname }}\n"
        dest: /etc/hostname
        owner: root
        group: root
        mode: '0644'

        ## install packages

    - name: Install required packages (Fedora)
      ansible.builtin.dnf:
        name:
          - ca-certificates
          - curl
          - gpg
          - htop
          - iotop
          - strace
          - lsof
          - mc
          - lynis
          - audit                  
          - btrfs-progs
          - git
          - rsync
          - plocate
          - jq
          - fastfetch              
          - firewalld
          - podman
          - podman-compose
          - slirp4netns
          - python3-podman
          - openssh-clients        
          - systemd-container
          - selinux-policy-targeted
          - policycoreutils
          - policycoreutils-python-utils
          - setools-console
          - checkpolicy
          - python3-bcrypt         
        state: present
        update_cache: yes

## Configure SELINUX

    # Ensure SELinux is enforcing with the targeted policy (idempotent).
    # The module reports if a reboot/relabel is required (e.g., if SELinux was disabled).
    - name: Ensure SELinux is enforcing (targeted)
      ansible.posix.selinux:
        policy: targeted
        state: enforcing
      register: selinux_result

    - name: Schedule full filesystem relabel if required
      ansible.builtin.file:
        path: /.autorelabel
        state: touch
      when: selinux_result.reboot_required | default(false)

    - name: Reboot to relabel if required
      ansible.builtin.reboot:
        reboot_timeout: 3600
      when: selinux_result.reboot_required | default(false)

    - name: Verify SELinux is enabled and enforcing
      ansible.builtin.command: getenforce
      register: getenforce_out
      changed_when: false

    - name: Switch to enforcing at runtime (if not already)
      ansible.builtin.command: setenforce 1
      when: getenforce_out.stdout != "Enforcing"
      failed_when: false

    # Podman: enable labeling and remind to use :Z on volume mounts
    - name: Ensure containers.conf enables labeling
      ansible.builtin.copy:
        dest: /etc/containers/containers.conf
        mode: '0644'
        content: |
          [containers]
          label = true

    - name: Reminder for Podman volumes
      ansible.builtin.debug:
        msg: >
          Use ':Z' on volume mounts in compose files (e.g. './data:/var/lib/nextcloud:Z')
          so host paths get the correct SELinux label automatically.

## configue firewalld

    - name: Ensure Firewalld is running and enabled
      ansible.builtin.systemd:
        name: firewalld
        enabled: yes
        state: started

    - name: Check existing Firewalld zones
      ansible.builtin.command: "firewall-cmd --get-zones"
      register: firewalld_existing_zones
      changed_when: false

    - name: Create Firewalld zones if missing
      ansible.builtin.firewalld:
        zone: "{{ item }}"
        permanent: yes
        state: present
      loop:
        - servers
        - 10G
        - containers
      when: "item not in firewalld_existing_zones.stdout"

    - name: Reload Firewalld to apply new zones
      ansible.builtin.command:
        cmd: "firewall-cmd --reload"
      changed_when: false

    - name: Ensure NetworkManager is running and enabled
      ansible.builtin.systemd:
        name: NetworkManager
        enabled: yes
        state: started

    # Persistently set the firewalld zone on the NM connection profile for each interface.
    # Idempotent: only changes when the current zone != desired zone.
    - name: Set NetworkManager connection.zone per interface
      ansible.builtin.shell: |
        set -e
        IFACE="{{ item.interface }}"
        ZONE="{{ item.zone }}"
        # Find the active NM connection name bound to this device
        CONN="$(nmcli -t -f NAME,DEVICE connection show --active | awk -F: -v d="$IFACE" '$2==d{print $1;exit}')"
        if [ -n "$CONN" ]; then
          CUR="$(nmcli -g connection.zone connection show "$CONN" || true)"
          if [ "$CUR" != "$ZONE" ]; then
            nmcli connection modify "$CONN" connection.zone "$ZONE"
            nmcli connection up "$CONN" >/dev/null 2>&1 || true
            echo changed
          fi
        else
          # No active NM profile for this device; nothing to modify here
          echo no_nm_profile
        fi
      args:
        executable: /bin/bash
      register: nm_zone_set
      changed_when: "'changed' in nm_zone_set.stdout"
      loop: "{{ zone_iface_map }}"
      loop_control:
        label: "{{ item.interface }} -> {{ item.zone }}"

    # Also bind interfaces directly in firewalld (harmless when NM already handles zones)
    - name: Ensure firewalld binds interfaces to desired zones
      ansible.builtin.firewalld:
        zone: "{{ item.zone }}"
        interface: "{{ item.interface }}"
        permanent: yes
        immediate: yes
        state: enabled
      loop: "{{ zone_iface_map }}"
      loop_control:
        label: "{{ item.interface }}"

    # Remove interfaces from the default 'public' zone so they only live in their target zone
    - name: Remove interfaces from public zone
      ansible.builtin.firewalld:
        zone: public
        interface: "{{ item.interface }}"
        permanent: yes
        immediate: yes
        state: disabled
      loop: "{{ zone_iface_map }}"

    - name: Reload firewalld
      ansible.builtin.command: firewall-cmd --reload
      changed_when: false   

    - name: Allow required services in Firewalld zones
      ansible.builtin.firewalld:
        zone: "{{ item.zone }}"
        service: "{{ item.service }}"
        permanent: yes
        immediate: yes
        state: enabled
      loop:
        - { zone: "servers", service: "ssh" }
        - { zone: "servers", service: "dns" }
        - { zone: "servers", service: "dhcp" }
        - { zone: "servers", service: "ntp" }
        - { zone: "10G",    service: "http" }
        - { zone: "10G",    service: "https" }
        - { zone: "10G",    service: "dns" }
        - { zone: "10G",    service: "dhcp" }
        - { zone: "10G",    service: "ntp" } 

    - name: Open required ports in Firewalld zones
      ansible.builtin.firewalld:
        zone: "{{ item.zone }}"
        port: "{{ item.port }}"
        permanent: yes
        immediate: yes
        state: enabled
      loop:
        - { zone: "containers", port: "8053/tcp" }
        - { zone: "containers", port: "8053/udp" }
        - { zone: "containers", port: "8081/tcp" }
        - { zone: "containers", port: "8082/tcp" }
        - { zone: "containers", port: "8083/tcp" }
        - { zone: "containers", port: "8084/tcp" }

    - name: Enable IP forwarding
      ansible.builtin.sysctl:
        name: net.ipv4.ip_forward
        value: '1'
        sysctl_set: yes
        state: present
        reload: yes

    - name: Redirect common ports for rootless podman (rich rules)
      ansible.posix.firewalld:
        zone: "{{ item.zone }}"
        rich_rule: 'rule family="ipv4" forward-port port="{{ item.src_port }}" protocol="{{ item.protocol }}" to-port="{{ item.to_port }}"'
        permanent: yes
        immediate: yes
        state: enabled
      loop:
        - { zone: "servers", src_port: 53,  protocol: "tcp", to_port: 8053 }
        - { zone: "servers", src_port: 53,  protocol: "udp", to_port: 8053 }
        - { zone: "servers", src_port: 67,  protocol: "tcp", to_port: 8067 }
        - { zone: "servers", src_port: 67,  protocol: "udp", to_port: 8067 }
        - { zone: "servers", src_port: 123, protocol: "udp", to_port: 8123 }
        - { zone: "10G",    src_port: 80,  protocol: "tcp", to_port: 8080 }
        - { zone: "10G",    src_port: 443, protocol: "tcp", to_port: 8443 }
        - { zone: "10G",    src_port: 53,  protocol: "tcp", to_port: 8053 }
        - { zone: "10G",    src_port: 53,  protocol: "udp", to_port: 8053 }
        - { zone: "10G",    src_port: 67,  protocol: "tcp", to_port: 8067 }
        - { zone: "10G",    src_port: 67,  protocol: "udp", to_port: 8067 }
        - { zone: "10G",    src_port: 123, protocol: "udp", to_port: 8123 }

    - name: Reload Firewalld to apply changes
      ansible.builtin.command:
        cmd: "firewall-cmd --reload"

## Set password and keys for admin user

    - name: Set michaelâ€™s Linux password
      ansible.builtin.user:
        name: michael
        password: "{{ michael_password | password_hash('sha512') }}"

    - name: Check if SSH private key exists for user michael
      ansible.builtin.stat:
        path: /home/michael/.ssh/id_rsa
      register: michael_ssh_key

    - name: Create .ssh directory for michael (if needed)
      ansible.builtin.file:
        path: /home/michael/.ssh
        state: directory
        owner: michael
        group: michael
        mode: '0700'

    - name: Generate SSH key for michael if it doesn't exist
      become_user: michael
      community.crypto.openssh_keypair:
        path: /home/michael/.ssh/id_rsa
        type: rsa
        size: 4096
        passphrase: "{{ ssh_key_passphrase }}"
        comment: "michael@{{ inventory_hostname }}"
      when: not michael_ssh_key.stat.exists

    - name: Read public SSH key
      ansible.builtin.slurp:
        src: /home/michael/.ssh/id_rsa.pub
      register: michael_public_key
      become_user: michael

    - name: Create /home/michael/build directory with proper ownership
      ansible.builtin.file:
        path: /home/michael/build
        state: directory
        owner: michael
        group: michael
        mode: '0755'

## Create bash aliases for root

    - name: Ensure aliases are present in .bashrc for root
      ansible.builtin.lineinfile:
        path: "{{ item.bashrc }}"
        line: "{{ item.alias }}"
        create: yes
        owner: "{{ item.user }}"
        group: "{{ item.user }}"
        mode: '0644'
        insertafter: EOF
      loop:
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias gs='git status'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias c='clear'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias h='history'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias ls='ls -alh --color=auto'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias df='df -h'" }
        - { user: 'root', bashrc: '/root/.bashrc', alias: "alias ports='ss -tulpn'" }

## Configure fastfetch for running after login

    - name: Make fastfetch run for every interactive shell
      ansible.builtin.copy:
        dest: /etc/profile.d/fastfetch.sh
        owner: root
        group: root
        mode: '0755'
        content: |
          #!/bin/sh
          # Only run in an interactive terminal
          if [ -t 1 ] && command -v fastfetch >/dev/null 2>&1; then
            fastfetch
          fi

## Configure podman user for running rootless containers

    - name: Ensure user 'podman' exists with a specific password
      ansible.builtin.user:
        name: podman
        shell: /bin/bash
        home: /home/podman

    - name: Set podmanâ€™s Linux password
      ansible.builtin.user:
        name: podman
        password: "{{ podman_password | password_hash('sha512') }}"

    - name: Ensure podman has authorized SSH key
      ansible.posix.authorized_key:
        user: podman
        state: present
        key: "{{ podman_ssh_pubkey }}"

    - name: Get podman user's UID
      ansible.builtin.command: id -u podman
      register: podman_uid_cmd
      changed_when: false

    - name: Set podman_uid fact
      ansible.builtin.set_fact:
        podman_uid: "{{ podman_uid_cmd.stdout }}"

    - name: Get podman user's GID
      ansible.builtin.command: id -g podman
      register: podman_gid_cmd
      changed_when: false

    - name: Set podman_gid fact
      ansible.builtin.set_fact:
        podman_gid: "{{ podman_gid_cmd.stdout }}"

    - name: Enable linger for podman user so socket survives reboot
      ansible.builtin.command: loginctl enable-linger podman

    - name: Ensure podman.socket is enabled and running for podman user
      ansible.builtin.command: >
        sudo -u podman env XDG_RUNTIME_DIR=/run/user/{{ podman_uid }}
        systemctl --user enable --now podman.socket

    - name: Ensure podman's bashrc exports DOCKER_HOST
      ansible.builtin.lineinfile:
        path: /home/podman/.bashrc
        line: 'export DOCKER_HOST=unix://$XDG_RUNTIME_DIR/podman/podman.sock'
        create: yes
        owner: podman
        group: podman
        mode: '0644'

    - name: Create systemd user unit directory
      ansible.builtin.file:
        path: /home/podman/.config/systemd/user
        state: directory
        owner: podman
        group: podman
        mode: '0755'

## Set podman registries

    - name: Deploy podman registries file
      ansible.builtin.template:
        src: container_server/podman/registries.conf.j2
        dest: /etc/registries.conf
        owner: root
        group: root
        mode: '0644'

    - name: Deploy podman user registries file
      ansible.builtin.template:
        src: container_server/podman/podman_registries.conf.j2
        dest: /home/podman/.config/containers/registries.conf
        owner: podman
        group: podman
        mode: '0644'

## Deploy podman containers

# NTP

    - name: Deploy NTP compose file
      ansible.builtin.template:
        src: container_server/compose/ntp/compose.yml.j2
        dest: /home/podman/compose/ntp/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start NTP
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/ntp.service
        content: |
          [Unit]
          Description=NTP

          [Service]
          WorkingDirectory=/home/podman/compose/ntp
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Nginx Proxy Manager

    - name: Deploy npm compose file
      ansible.builtin.template:
        src: container_server/compose/npm/compose.yml.j2
        dest: /home/podman/compose/npm/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Deploy npm env file
      ansible.builtin.template:
        src: container_server/compose/npm/.env.j2
        dest: /home/podman/compose/npm/.env
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Nginx Proxy Manager
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/npm.service
        content: |
          [Unit]
          Description=Nginx Proxy Manager

          [Service]
          WorkingDirectory=/home/podman/compose/npm
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Heimdall

    - name: Deploy Heimdall compose file
      ansible.builtin.template:
        src: container_server/compose/heimdall/compose.yml.j2
        dest: /home/podman/compose/heimdall/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Heimdall
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/heimdall.service
        content: |
          [Unit]
          Description=Heimdall

          [Service]
          WorkingDirectory=/home/podman/compose/heimdall
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Pihole-clients

    - name: Deploy Pihole-clients compose file
      ansible.builtin.template:
        src: container_server/compose/pihole-clients/compose.yml.j2
        dest: /home/podman/compose/pihole-clients/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Deploy Pihole-clients env file
      ansible.builtin.template:
        src: container_server/compose/pihole-clients/.env.j2
        dest: /home/podman/compose/pihole-clients/.env
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Pihole-clients
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/pihole-clients.service
        content: |
          [Unit]
          Description=Pihole-clients

          [Service]
          WorkingDirectory=/home/podman/compose/pihole-clients
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

# Pihole-servers

    - name: Deploy Pihole-servers compose file
      ansible.builtin.template:
        src: container_server/compose/pihole-servers/compose.yml.j2
        dest: /home/podman/compose/pihole-servers/compose.yml
        owner: podman
        group: podman
        mode: '0644'

    - name: Deploy Pihole-servers env file
      ansible.builtin.template:
        src: container_server/compose/pihole-servers/.env.j2
        dest: /home/podman/compose/pihole-servers/.env
        owner: podman
        group: podman
        mode: '0644'

    - name: Install systemd unit to start Pihole-servers
      ansible.builtin.copy:
        dest: /home/podman/.config/systemd/user/pihole-servers.service
        content: |
          [Unit]
          Description=Pihole-clients

          [Service]
          WorkingDirectory=/home/podman/compose/pihole-servers
          ExecStart=/usr/bin/podman-compose up
          ExecStop=/usr/bin/podman-compose down
          Restart=always
          TimeoutStartSec=0
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=default.target
        owner: podman
        group: podman
        mode: '0644'

    # ## Reload systemd and enable/start containers

#     - name: Reload systemd user units for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user daemon-reload'

# # NTP Server

#     - name: Enable NTP service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable ntp.service'

# # Nginx Proxy Manager

#     - name: Enable NPM service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable npm.service'

# # Heimdall

#     - name: Enable Heimdall service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable heimdall.service'

# # Pihole-clients

#     - name: Enable Pihole-clients service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable pihole-clients.service'

# # Pihole-servers

#     - name: Enable Pihole-servers service for podman
#       ansible.builtin.command: machinectl shell podman@ /bin/bash -c 'systemctl --user enable pihole-servers.service'

## Finishing deployment by rebooting the server

    - name: Reboot the server
      ansible.builtin.reboot:
        reboot_timeout: 600     # wait up to 10 minutes for reboot
        test_command: whoami    # command to verify SSH is back
      tags: reboot

    - name: Reset SSH connection after reboot
      meta: reset_connection
      tags: reboot